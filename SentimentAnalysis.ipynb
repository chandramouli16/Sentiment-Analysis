{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary lib \n",
    "# using the twitter tweets samples from nltk package\n",
    "\n",
    "#Resources :\n",
    "\n",
    "#nltk.download('twitter_samples')\n",
    "#The punkt module is a pre-trained model that helps you tokenize words and sentences\n",
    "#nltk.download('punkt')\n",
    "#wordnet is a lexical database for the English language that helps the script determine the base word. You need the averaged_perceptron_tagger resource to determine the context of a word in a sentence.\n",
    "#nltk.download('wordnet')\n",
    "#nltk.dowmload('averaged_perceptron_tagger')\n",
    "#nltk.download('stopwords')\n",
    "# find out which are the most common words using the FreqDist class of NLTK\n",
    "\n",
    "import re,string,random\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples ,stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, create variables for positive_tweets, negative_tweets\n",
    "#The strings() method of twitter_samples will print all of the tweets within a dataset as strings\n",
    "stop_words = stopwords.words('english')\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "\n",
    "#The first part of making sense of the data is through a process called tokenization, or splitting strings into smaller parts called tokens.A token is a sequence of characters in text that serves as a unit\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['#FollowFriday',\n",
       " '@France_Inte',\n",
       " '@PKuchly57',\n",
       " '@Milipol_Paris',\n",
       " 'for',\n",
       " 'be',\n",
       " 'top',\n",
       " 'engage',\n",
       " 'member',\n",
       " 'in',\n",
       " 'my',\n",
       " 'community',\n",
       " 'this',\n",
       " 'week',\n",
       " ':)']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3 — Normalizing the Data \n",
    "#Words have different forms—for instance, “ran”, “runs”, and “running” are various forms of the same verb, “run”.\n",
    "# Normalization in NLP is the process of converting a word to its canonical form.\n",
    "\n",
    "#  types : stemming and lemmatization\n",
    "print(pos_tag(tweet_tokens[0]))\n",
    "\n",
    "def lemmatizer_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word,tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word,pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "lemmatizer_sentence(tweet_tokens[0])\n",
    "# to know the meaning of tags : https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "#In general, if a tag starts with NN, the word is a noun and if it stars with VB, the word is a verb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# Step 4 — Removing Noise from the Data\n",
    "\n",
    "#Noise is any part of the text that does not add meaning or information to data.\n",
    "\n",
    "'''\n",
    "Hyperlinks - All hyperlinks in Twitter are converted to the URL shortener t.co. Therefore, keeping them in the text processing would not add any value to the analysis.\n",
    "\n",
    "Twitter handles in replies - These Twitter usernames are preceded by a @ symbol, which does not convey any meaning.\n",
    "\n",
    "Punctuation and special characters - While these often provide context to textual data, this context is often difficult to process. For simplicity, you will remove all punctuation and special characters from tweets.\n",
    "'''\n",
    "\n",
    "def remove_noise(tweet_tokens,stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "            \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token,pos)\n",
    "        \n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "print(remove_noise(tweet_tokens[0],stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top positive words :  [(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n",
      "\n",
      "top negative words :  [(':(', 4585), (':-(', 501), (\"i'm\", 343), ('...', 332), ('get', 325), ('miss', 291), ('go', 275), ('please', 275), ('want', 246), ('like', 218)]\n"
     ]
    }
   ],
   "source": [
    "#Step 5 — Determining Word Density\n",
    "#The most basic form of analysis on textual data is to take out the word frequency.\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "            \n",
    "all_positive_words = get_all_words(positive_cleaned_tokens_list)\n",
    "all_negative_words = get_all_words(negative_cleaned_tokens_list)\n",
    "\n",
    "\n",
    "# to know most occured words using freqdist\n",
    "freq_dist_pos = FreqDist(all_positive_words)\n",
    "freq_dist_neg = FreqDist(all_negative_words)\n",
    "\n",
    "print(\"top positive words : \",freq_dist_pos.most_common(10))\n",
    "print(\"\\ntop negative words : \",freq_dist_neg.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6 — Preparing Data for the Model\n",
    "\n",
    "'''\n",
    "Converting Tokens to a Dictionary\n",
    "\n",
    "First, you will prepare the data to be fed into the model. \n",
    "You will use the Naive Bayes classifier in NLTK to perform the modeling exercise. \n",
    "Notice that the model requires not just a list of words in a tweet, but a Python dictionary with words as keys and True as values. \n",
    "The following function makes a generator function to change the format of the cleaned data.'''\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token ,True] for token in tweet_tokens)\n",
    "    \n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the Dataset for Training and Testing the Model\n",
    "\n",
    "positive_dataset = [(tweet_dict,\"Positive\") for tweet_dict in positive_tokens_for_model]\n",
    "negative_dataset = [(tweet_dict,\"Negative\") for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset+negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  0.996\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2066.0 : 1.0\n",
      "                followed = True           Negati : Positi =     22.9 : 1.0\n",
      "                follower = True           Positi : Negati =     21.9 : 1.0\n",
      "                     sad = True           Negati : Positi =     19.8 : 1.0\n",
      "                    glad = True           Positi : Negati =     19.1 : 1.0\n",
      "                    luck = True           Positi : Negati =     15.1 : 1.0\n",
      "                     x15 = True           Negati : Positi =     14.9 : 1.0\n",
      "              appreciate = True           Positi : Negati =     14.4 : 1.0\n",
      "                  arrive = True           Positi : Negati =     13.7 : 1.0\n",
      "                    miss = True           Negati : Positi =     12.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Step 7 — Building and Testing the Model\n",
    "\n",
    "'''Finally, you can use the NaiveBayesClassifier class to build the model. \n",
    "Use the .train() method to train the model and the .accuracy() method to test the model on the testing data.'''\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is : \",classify.accuracy(classifier,test_data))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "#Next, you can check how the model performs on random tweets from Twitter\n",
    "\n",
    "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "\n",
    "custom_token = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token,True] for token in custom_token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies\"\n",
    "\n",
    "custom_token = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token,True] for token in custom_token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
